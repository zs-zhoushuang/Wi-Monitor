Wi-Monitor: Daily Activity Monitoring Using Commodity Wi-Fi submitted to IEEE Internet of Things Journal

The test data and training model can be download in https://drive.google.com/file/d/1rngGXwK7SB1LiB5dje1RtByFv21VMQIc/view?usp=sharing.

Put the two folders: Params_basic_scenario, and TestData_basic_scenario into the basic_scenario folder, and you can run the test codes to show the performance under the basic scenario.

Put the two folders: Params_cross_subject_scenario, and TestData_cross_subject_scenario into the cross_subject_scenario folder, and you can run the test codes to show performance under the cross-subject scenario.

Run the calculate_ave_acc.py and you can get the average performance under the basic scenario and the cross-subject scenario, respectively.

Put the two folders: Params_cross_environment_scenario, and TestData_cross_environment_scenario into the cross_environment_scenario folder, and you can run the test code to show the performance under the cross-environment scenario.

Run the calculate_crossEnvir_ave_acc.py and you can get the average performance under the cross_environment scenario.


Here are some challenges and solutions we faced during the combination of feature extraction network and Temporal Convolutional Network (TCN) to capture ACFs as distinguishing characteristics of continuous activities.

(1) How to extract activity features from the continuous activities as the input of TCN is a challenge. First of all, we note that extracting activity features from continuous activities as the input of TCN is indispensable. The reasons are as follows. On the one hand, if we directly input the CSI stream of continuous activities into TCN to capture ACFs, it is equivalent that the AFF sequence length of an activity is equal to the CSI package length of this activity, which will dramatically burden TCN to capture ACFs of this activity for monitoring. On the other hand, the data of one CSI package is too little to include sufficient human body information, which greatly increases the difficulty for TCN to predict the activity category of each package. Both of these aspects will dramatically degrade the performance of continuous activity monitoring. Thus, in our system, we extract activity features from the continuous activities first and input them into TCN to capture ACFs.

However, continuous activities have unknown start/end points and different durations, which make it impossible to downsample/upsample each activity to a fixed window or to select a window directly, whose length corresponds to each activity, and then use a network to extract activity features from these windows as the input of TCN. To address this challenge, we first fragmentize a pure CSI stream (obtained after signal preprocessing) into CSI bins with the same short size in time order aiming at cutting activities embedded in the stream into fragments of the same short duration. Then, we design a feature extraction network, in which we use several residual convolutional blocks to extract features (called AFFs in our paper) from these fragments and convert them into AFF vectors using a fully connected layer. Finally, we arrange all AFF vectors in time order as an AFF sequence. Through the above operations, the AFF sequence can not only include activity features but also preserve the temporal nature. Thus, we use it as the activity features of continuous activities and input it into TCN to capture ACFs. 

(2) How to capture long-range latent dependencies (called ACFs in our paper) from the AFF sequence as the distinguishing characteristics of continuous activities is another challenge. Intuitively, when a human performs continuous activities, his/her body movements are correlated in the time sequence. Thus, capturing long-range latent dependencies of continuous activities is indispensable for distinguishing different activities and achieving accurate activity monitoring. However, capturing long-range latent dependencies from the AFF sequence using conventional sequence models is still challenging. The reason is that Long Short-Term Memories (LSTMs), which are the most effective deep learning-based sequence models at present, are still limiting when activities are correlated with human movements over a long time. This is because the latent state of LSTMs at each time step t is only a function of the data at t, and the hidden state and memory at step t-1. This makes it difficult for them to capture long-range latent dependencies from the AFF sequence which includes human movement relations over a long time.

To address this challenge, we design the TCN based on several TCN blocks to capture long-range latent dependencies from the AFF sequence. The TCN blocks have been proposed and proved that they outperform recurrent architectures, e.g. Long Short-Term Memories (LSTMs), across a broad range of sequence modeling tasks, recently. A TCN block is mainly composed of two dilated causal convolutions and a residual connection. The dilated causal convolution is designed to solve sequence tasks better using convolutional architecture and can obtain the exponentially large receptive field corresponding to the network layers. Note that the receptive field is equal to the memory in convolutional architectures. In addition, the residual connection is designed to deepen the networks because it can effectively handle the degradation issue of deep networks. Therefore, the TCN we constructed deeply with several TCN blocks can exhibit an extremely long memory to capture long-range latent dependencies from the AFF sequence. We call these long-range latent dependencies ACFs in our paper and use them as distinguishing characteristics of continuous activities for accurate activity monitoring.
